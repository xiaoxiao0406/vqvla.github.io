# VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers
