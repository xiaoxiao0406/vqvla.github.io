<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers</title>
  <link rel="icon" type="image/x-icon" href="static/images/robot.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VQ-VLA: Improving Vision-Language-Action Models via Scaling
              Vector-Quantized Action Tokenizers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=5SuBWh0AAAAJ" target="_blank">Yating
                  Wang</a><sup>12</sup>,</span>
              <span class="author-block">
                <a href="https://www.haoyizhu.site/" target="_blank">Haoyi Zhu</a><sup>13</sup>,</span>
              <span class="author-block">
                <a href="https://mingyulau.github.io/" target="_blank">Mingyu Liu</a><sup>14</sup>,</span>
              <span class="author-block">
                <a href="https://yangjiangeyjg.github.io/" target="_blank">Jiange Yang</a><sup>15</sup>,</span>
              <span class="author-block">
                <a href="https://fang-haoshu.github.io/" target="_blank">Hao-Shu Fang</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="http://tonghe90.github.io/" target="_blank">Tong He</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Shanghai Artificial Intelligence Laboratory, <sup>2</sup>Tongji
                University, <sup>3</sup>University of Science and Technology of China, <sup>4</sup>Zhejiang University,
                <sup>5</sup>Nanjing University, <sup>6</sup>Shanghai Jiao Tong University <br><span
                  class="highlight-orange mt-2" style="display: inline-block;">ICCV 2025</span></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/VQ-VLA.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/xiaoxiao0406/VQ-VLA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2507.01016>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/videos/demo_v2.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat
          pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
        </h2>
      
      <!-- </div> -->
        <div class="item">
          <img src="static/images/vqvla_pipeline.png" alt="Method Overview" class="center-image blend-img-background" />
        </div>

        <div class="content has-text-justified">
          <p>
            <span class="highlight-orange">VQ-VLA</span> is an innovative vector quantization based action tokenizer
            built upon the largest-scale action
            trajectory
            dataset to date, leveraging over 100 times more data than previous approaches. It demonstrates that action
            tokenizers
            can be effectively scaled by leveraging large-scale simulated action
            data. We prove that our action
            tokenizers
            improve
            the performance, inference speed, and long-horizon capabilities of
            VLA models
          </p>
        </div>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-nine-tenths">
          <h2 class="title is-3"><span class="highlight-orange">Abstract</span></h2>
          <div class="content has-text-justified">
            <p>
              In this paper, we introduce an innovative vector quantization based action tokenizer built upon the
              largest-scale action
              trajectory dataset to date, leveraging over 100 times more data than previous approaches. This extensive
              dataset enables
              our tokenizer to capture rich spatiotemporal dynamics, resulting in a model that not only accelerates
              inference but also
              generates smoother and more coherent action outputs. Once trained, the tokenizer can be seamlessly adapted
              to a wide
              range of downstream tasks in a zero-shot manner, from short-horizon reactive behaviors to long-horizon
              planning. A key
              finding of our work is that the domain gap between synthetic and real action trajectories is marginal,
              allowing us to
              effectively utilize a vast amount of synthetic data during training without compromising real-world
              performance. To
              validate our approach, we conducted extensive experiments in both simulated environments and on real
              robotic platforms.
              The results demonstrate that as the volume of synthetic trajectory data increases, the performance of our
              tokenizer on
              downstream tasks improves significantlyâ€”most notably, achieving up to a 30% higher success rate on two
              real-world tasks
              in long-horizon scenarios.These findings highlight the potential of our action tokenizer as a robust and
              scalable
              solution for real-time embodied intelligence systems, paving the way for more efficient and reliable
              robotic control in
              diverse application domains.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Real World Setup -->

  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-nine-tenths">
          <h2 class="title is-3"><span class="highlight-orange">Real-World Setup</span></h2>
          <div class="content has-text-justified">
            <p>
              For real robot experiments, we use a Franka Research3 arm with a fixed RealSense D435 camera to capture
              environmental
              observations. We evaluate on six manipulation tasks (4 short-horizon, 2 long-horizon) designed to evaluate
              the model's ability to handle varying task complexities. For each task, we collect 50 demonstrations and
              evaluate performance over 20 trials.
            </p>
          </div>

          <div class="item">
            <img src="static/images/real_world_setup.png" alt="Real World Setup"
              class="center-image blend-img-background" />
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End youtube video -->

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-nine-tenths">
          <h2 class="title is-3"><span class="highlight-orange">Demo</span></h2>
          <video poster="" id="tree" autoplay controls muted loop height="100%">
            <source src="static/videos/demo_v2.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </section>

  <!-- Results -->
  <section class="section hero is-small">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-nine-tenths">
          <h2 class="title is-3"><span class="highlight-orange">Results</span></h2>
          <!-- Method overview image -->
          <div class="item">
            <img src="static/images/real_world_result.png" alt="Real World Results"
              class="center-image blend-img-background" />
          </div>
          <div class="content has-text-justified mt-4">
            <p>
              Adding synthetic trajectories significantly boosts the action tokenizer's performance, raising the average
              success rate
              from 23% to 46.25%, with notable gains in precision and dynamic tasks like "Flip the pot upright" (+30%)
              and "Pull out a
              tissue paper" (from 5% to 20%+). While the limited LIBERO dataset showed minimal impact on short-horizon
              tasks, the much
              larger ManiSkill dataset led to substantial improvements. For long-horizon tasks, VQ-VLA, especially with
              VQ<sub>O+L+M</sub>,
              dramatically outperforms baselines, achieving success rates of 50% and 30% on complex tasks where
              baselines struggle.
              This is largely due to VQ-VAE's ability to predict multiple actions per inference, reducing error
              accumulation and
              improving efficiency in long, sequential tasks.
            </p>
          </div>

          <hr class="my-5" style="background-color: #dbdbdb; height: 1px; border: none;">
          <div class="item">
            <img src="static/images/inference.png" alt="inference time" class="center-image blend-img-background" />
          </div>
          <div class="content has-text-justified mt-4">
            <p>
              During the real-world experiments, we measured the action execution frequency of VQ-VLA and compared it
              with the
              original OpenVLA. As shown in the table, with a
              compression
              ratio of 5 in VQ-VAE, the inference speed is nearly tripled. This significant improvement greatly
              facilitates real-time
              performance in practical applications.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->



  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title"><span class="highlight-orange"></span>BibTeX</span></h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>